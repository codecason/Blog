#### Noise Contrastive Estimation 前世今生——从 NCE 到 InfoNCE

https://zhuanlan.zhihu.com/p/334772391



$$
D_W(\overrightarrow{X_1},\overrightarrow{X_2})=||G_W(\overrightarrow{X_1})-G_W(\overrightarrow{X_2})||_2
$$
Noise 对比学习

Noise Contrastive Estimation（噪声对比估计）

#### 论文1的对比学习

NCE，也就是 Noise Contrastive Estimation（噪声对比估计）， 在 [2] 这篇论文中被提出，但是这篇论文的阐述的不太便于理解，并且论文中估计的是概率密度函数（pdf, probability density function）。而 NLP 中的 word 或 vision 中的 pixel 都是离散的，且我们感兴趣的是的概率质量函数（pmf, probability mass function），因此我主要参考了 [4] 这篇论文，它就是在使用 NCE 时假设了离散分布，并用 pmf 代替其中 pdf，然后将 NCE 应用到 NLP 领域。



相较定义来说，Triplet Loss认为，假如所有正样本之间无限的拉近，会导致聚类过拟合，所以，就只要求
$$
d(x,x^-)>d(x,x^+)+\alpha
$$


当然在比例尺上看来， $d(x,x^+)$ 也会趋于0。

原文将所有三元组的状态分为三类：

- **hard triplets**
  正样本离锚点的距离比负样本还大
- **semi-hard triplets**
  正样本离锚点的距离比负样本小，但未满足​
- **easy triplets**

满足$d(x, x^-)>d(x, x^+)+\alpha$

前两个状态会通过loss逐渐变成第三个状态。

#### NCE Loss

在正例和负例中，

NCE, loss=Noise Contrastive Estimation

$c$是上下文，而公式里的函数是训练分布（概率模型）$p_\theta(w|c)=\sum\frac{1}{Z(c)}exp(u_\theta(x))$；经验分布是$\tilde{p}(w|c)$


$$
\begin{align*}
k=\frac{num(x^-)}{num(x^+)} \\
J_{NCE}^c &= \mathbb E_{w\sim\tilde{p}(x|c)}log\frac{u_\theta(w,c)}{u_\theta(w,c)+kq(w)}+k\mathbb{E}_{w\sim{q(w)}}log\frac{kq(w)}{u_\theta(w,c)+kq(w)}J_{NCE} \\
 &= \sum_cP(c)J_{NCE}^c
\end{align*}
$$

$$
p(D=0|w,c)=\frac{k\times q(w)}{\tilde{p}(w\mid c)+k\times q(w)}  \\
p(D=1|w,c)=\frac{\tilde{p}(w\mid c)}{\tilde{p}(w\mid c)+k\times q(w)}
$$



NCE 将问题转换成了一个二分类问题，分类器能够对数据样本和噪声样本进行二分类，而这个分类器的参数 $\theta$ 就等价于我们想要得到 的$\theta$。

NCE=q(x)+p(x)+网络架构+二分类+D_t+{Z(c)=1}=是一个目标函数

参数调整=k尽量大+噪声分布可以随意

二分类=训练分布+噪声分布

证明了最小化 InfoNCE 损失实际上是在最大化表示与正例之间的互信息（MI）。

InfoNCE=NCE+交叉熵损失+互信息




$$
I(x_{t+k};c_t)=\sum_{x,c}p(x_{t+k},c_t)\log\frac{p(x_{t+k}\mid c_t)}{p(x_{t+k})}
$$

$$
\begin{gathered}
\mathcal{L}_{N}=-\sum_X\left[p(x,c)\log\frac{f_k\left(x_{t+k},c_t\right)}{\sum_{x_j\in X}f_k\left(x_j,c_t\right)}\right] \\
=-\mathbb{E}_X\left[\log\frac{f_k\left(x_{t+k},c_t\right)}{\sum_{x_j\in X}f_k\left(x_j,c_t\right)}\right]
\end{gathered}
$$


判断题

由于深度学习的出现，特征工程也不再必要了。

$Z_\theta(x)=\sum_x{u_\theta(x)}$,若 x 是 continuous space， 但是计算$Z_\theta$的积分没有公式解时，就无法继续进行下去。 (不然就要用 sampling 方法, 如 MCMC)

nce为什么可以将归一化项Z置为1，而softmax不行，，这是最关键的，但是本文好像没有重点解释这个，一句话就跳过了。



#### References

https://zhuanlan.zhihu.com/p/346686467

https://bobondemon.github.io/2021/06/05/Noise-Contrastive-Estimation-NCE-%E7%AD%86%E8%A8%98/

[Representation Learning with Contrastive Predictive Coding](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1807.03748)

[Dimensionality Reduction by Learning an Invariant Mapping](https://ieeexplore.ieee.org/document/1640964)
